{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard-coding the gradient in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R7bbcaOFtDzu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bjcp7_0ltfhD"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "T = iris.target\n",
    "\n",
    "C= len(set(list(T)))\n",
    "F = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IHsMk5a-tlpL"
   },
   "outputs": [],
   "source": [
    "x=X[0]\n",
    "t=T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZJ3dek7t-zs"
   },
   "source": [
    "So we loaded the  x s in variable X and the labels  t  in variable T. \n",
    "We also calculate the number of all possible classes C (in our case the classes are {0,1,2} so C=3) and the number of dimensions (features) in each datapoint F (in our case F=4). Now it's always a good idea to compute one term of the neg. log likelihood before we scale up to the whole thing. So let's define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7SBA0VTAtpW8"
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(C,F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJRiJ6l1uO3e"
   },
   "source": [
    "It will be very convenient to store the weights of the model in a single array `W` which has `C` rows and `F` columns. Let's initialize it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DWuK_-zUts4M"
   },
   "outputs": [],
   "source": [
    "p = np.exp(W @ x)\n",
    "y = p/sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdeTfQCyufSl"
   },
   "source": [
    "The matrix multiplication $W \\mathbf{x}$ gives the linear activations, i.e. a vector with elements $\\mathbf{w}_1^T \\mathbf{x},\\ldots,\\mathbf{w}_C^T \\mathbf{x}$ before they get passed to the soft-max function. The soft-max vector itself can be computed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT9ofGSfulk1",
    "outputId": "de310494-6f26-4260-b1b6-290664da40be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.41194071e-01 4.83613545e-05 7.58757567e-01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjc2aBuwuuOe"
   },
   "source": [
    "Let's verify that it is a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eXGHS2t3uv1O"
   },
   "outputs": [],
   "source": [
    "L = - np.log(y[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1tOylQEu3UO"
   },
   "source": [
    "Now the neg. log likelihood just for this datapoint can be defined as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "utbqROUKu6M_"
   },
   "outputs": [],
   "source": [
    "dW = np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZcrcWJsvWCY"
   },
   "source": [
    "Let's now define a variable to hold the gradient of $L$ with respect to the weights, i.e. $\\nabla_W L$ which we will initialize to zero. Note that it has exactly the same dimensions as $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2qzpX_MDvfh5"
   },
   "outputs": [],
   "source": [
    "# According to the formula we derived above\n",
    "for c in range(C): #we need to do is loop through the whole dataset, adding the gradients as we go along.\n",
    "    dW[c] = (y[c] - 1) * x if c==t else y[c] * x #We will finally update the weights by adding a small negative multiple of the gradient, which is guaranteed to decrease the total neg. log likelihood by a small amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM6WpR63wqhg"
   },
   "source": [
    "It's also a good idea for sanity checking and general good practice, to produce some plot of the (hopefully) decreasing  L . Adding everything together we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "hDVZGsA7vtbo",
    "outputId": "697728bb-ad86-46ef-e7b2-22ebb5f8d0a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe326413790>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ls=[] # this array will hold the neg. log likelihood as it decreases throughout \n",
    "# the algorithm\n",
    "W = np.random.randn(C,F) # Our model weights, randomly initialized\n",
    "alpha = 5e-4 # The learning rate\n",
    "for e in range(1000): # We will perform 1000 steps of gradient descent\n",
    "  dW = np.zeros_like(W) #initialize gradient to zero\n",
    "  L=0 #initialize the neg.log likelihood sum\n",
    "  for x,t in zip(X,T): #loop through dataset\n",
    "    p = np.exp(W @ x) #soft-max\n",
    "    y = p/sum(p) \n",
    "    L += -np.log(y[t]) #add neg. log. likelihood for datapoint\n",
    "    for c in range(C):# compute gradient for datapoint\n",
    "        dW[c] += (y[c] - 1) * x if c==t else y[c] * x    \n",
    "  W -= alpha*dW # move weights in direction oposite to gradient\n",
    "  Ls.append(L) # keep record of neg. log likelihood\n",
    "plt.plot(Ls,'r-') # plot trajectory of L during gradient descent **V. IMPORTANT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvEDWbe_xn3B"
   },
   "source": [
    "Seems that we managed to decrease something! Let's see if this corresponds to some accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xMGZOtGxnBX",
    "outputId": "944909fc-14c7-4f8e-e2f2-4fcb294729f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for x,t in zip(X,T):\n",
    "  a_pred = W @ x\n",
    "  correct += (a_pred.argmax() == t)\n",
    "print(f'accuracy={correct/len(T)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNypydOGx9dI"
   },
   "source": [
    "That means that a pretty descent 97% of the class of all datapoints is correctly predicted by our model. (It's a pretty simple dataset after all)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab2onGD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
