{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating gradient descent using PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-HbinQQyOeu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "import torch\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = torch.tensor(iris.data,dtype=torch.float32)\n",
    "T = torch.tensor(iris.target,dtype=torch.long)\n",
    "\n",
    "C= len(set(list(T)))\n",
    "F = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhFPh4e-y-UA"
   },
   "outputs": [],
   "source": [
    "#Let us also initialize the weight matrix $W$ as a $C\\times F$ random `Pytorch` matrix. \n",
    "W= torch.randn(C,F,requires_grad=True,dtype=torch.float32)\n",
    "#Only difference from before is this mysterious option flag requires_grad that is set to True. We will see what this does shortly. As before, we will focus on a single datapoint  x,t ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iAo-i0c0DuH"
   },
   "outputs": [],
   "source": [
    "x=X[0]\n",
    "t=T[0]\n",
    "p=torch.exp(W @ x)\n",
    "y = p/sum(p)\n",
    "L = -torch.log(y[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVK5iix_0SoQ"
   },
   "source": [
    "Again, notice how this is all identical to the equivalent numpy code above. The only difference being that we must call the tensor versions of exp, log etc because we are dealing with Pytorch tensors instead of numpy arrays.\n",
    "\n",
    "Now instead of going to the trouble of computing the gradient of  L  with respect to  W , we only have to execute a single (magic) line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1HVt30o0VVw"
   },
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHyvjL0b0jxx"
   },
   "source": [
    "and we're done! Of course you will ask, where is the gradient? We didn't define any dW variable! The answer is that Pytorch, very conveniently automatically calculates the gradient and hides it in a specially defined .grad attribute. So in our case the gradient can be found in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mW5mMU10mV3",
    "outputId": "17f77628-af38-40c8-fcaf-45e4323f81a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1000e+00, -3.5000e+00, -1.4000e+00, -2.0000e-01],\n",
      "        [ 4.2820e-10,  2.9386e-10,  1.1755e-10,  1.6792e-11],\n",
      "        [ 4.4988e-07,  3.0874e-07,  1.2350e-07,  1.7643e-08],\n",
      "        [ 1.5980e-10,  1.0967e-10,  4.3867e-11,  6.2667e-12],\n",
      "        [ 5.2638e-04,  3.6124e-04,  1.4450e-04,  2.0642e-05],\n",
      "        [ 1.7278e-07,  1.1858e-07,  4.7431e-08,  6.7759e-09],\n",
      "        [ 7.1163e-05,  4.8837e-05,  1.9535e-05,  2.7907e-06],\n",
      "        [ 1.0841e-10,  7.4401e-11,  2.9760e-11,  4.2515e-12],\n",
      "        [ 8.2461e-10,  5.6591e-10,  2.2636e-10,  3.2338e-11],\n",
      "        [ 4.5206e-08,  3.1024e-08,  1.2409e-08,  1.7728e-09],\n",
      "        [ 1.0777e-06,  7.3958e-07,  2.9583e-07,  4.2262e-08],\n",
      "        [ 5.1212e-09,  3.5146e-09,  1.4058e-09,  2.0083e-10],\n",
      "        [ 3.1028e-10,  2.1294e-10,  8.5174e-11,  1.2168e-11],\n",
      "        [ 1.1105e-04,  7.6211e-05,  3.0484e-05,  4.3549e-06],\n",
      "        [ 5.5803e-07,  3.8296e-07,  1.5318e-07,  2.1883e-08],\n",
      "        [ 2.0686e-08,  1.4196e-08,  5.6786e-09,  8.1122e-10],\n",
      "        [ 4.4242e-06,  3.0362e-06,  1.2145e-06,  1.7350e-07],\n",
      "        [ 1.5503e-08,  1.0639e-08,  4.2558e-09,  6.0797e-10],\n",
      "        [ 1.6165e-06,  1.1094e-06,  4.4374e-07,  6.3392e-08],\n",
      "        [ 1.7365e-05,  1.1917e-05,  4.7669e-06,  6.8099e-07],\n",
      "        [ 3.1929e-07,  2.1912e-07,  8.7647e-08,  1.2521e-08],\n",
      "        [ 9.8403e-08,  6.7532e-08,  2.7013e-08,  3.8589e-09],\n",
      "        [ 4.7511e-11,  3.2606e-11,  1.3042e-11,  1.8632e-12],\n",
      "        [ 2.3154e-09,  1.5890e-09,  6.3561e-10,  9.0801e-11],\n",
      "        [ 5.0313e-06,  3.4529e-06,  1.3811e-06,  1.9731e-07],\n",
      "        [ 5.4559e-09,  3.7442e-09,  1.4977e-09,  2.1396e-10],\n",
      "        [ 2.9372e-08,  2.0157e-08,  8.0628e-09,  1.1518e-09],\n",
      "        [ 1.5637e-11,  1.0732e-11,  4.2926e-12,  6.1324e-13],\n",
      "        [ 1.1013e-06,  7.5580e-07,  3.0232e-07,  4.3189e-08],\n",
      "        [ 2.1964e-13,  1.5074e-13,  6.0294e-14,  8.6135e-15],\n",
      "        [ 9.4828e-08,  6.5078e-08,  2.6031e-08,  3.7188e-09],\n",
      "        [ 1.4998e-07,  1.0293e-07,  4.1172e-08,  5.8818e-09],\n",
      "        [ 4.2036e-05,  2.8848e-05,  1.1539e-05,  1.6485e-06],\n",
      "        [ 3.3252e-09,  2.2820e-09,  9.1279e-10,  1.3040e-10],\n",
      "        [ 2.1303e-05,  1.4619e-05,  5.8477e-06,  8.3539e-07],\n",
      "        [ 5.9981e-08,  4.1164e-08,  1.6465e-08,  2.3522e-09],\n",
      "        [ 1.1262e-07,  7.7287e-08,  3.0915e-08,  4.4164e-09],\n",
      "        [ 3.4784e-09,  2.3871e-09,  9.5484e-10,  1.3641e-10],\n",
      "        [ 1.7320e-04,  1.1886e-04,  4.7546e-05,  6.7922e-06],\n",
      "        [ 2.1173e-06,  1.4530e-06,  5.8121e-07,  8.3030e-08],\n",
      "        [ 1.8245e-08,  1.2521e-08,  5.0085e-09,  7.1549e-10],\n",
      "        [ 4.0609e-11,  2.7869e-11,  1.1148e-11,  1.5925e-12],\n",
      "        [ 1.6400e-04,  1.1255e-04,  4.5020e-05,  6.4314e-06],\n",
      "        [ 7.4610e-06,  5.1203e-06,  2.0481e-06,  2.9259e-07],\n",
      "        [ 1.2379e-08,  8.4953e-09,  3.3981e-09,  4.8544e-10],\n",
      "        [ 2.6368e-05,  1.8096e-05,  7.2383e-06,  1.0340e-06],\n",
      "        [ 1.8346e-10,  1.2590e-10,  5.0362e-11,  7.1945e-12],\n",
      "        [ 1.6957e-11,  1.1637e-11,  4.6547e-12,  6.6496e-13],\n",
      "        [ 2.8295e-04,  1.9418e-04,  7.7674e-05,  1.1096e-05],\n",
      "        [ 3.3628e-10,  2.3078e-10,  9.2313e-11,  1.3188e-11],\n",
      "        [ 1.2026e-08,  8.2535e-09,  3.3014e-09,  4.7163e-10],\n",
      "        [ 6.4768e-13,  4.4448e-13,  1.7779e-13,  2.5399e-14],\n",
      "        [ 9.6507e-05,  6.6230e-05,  2.6492e-05,  3.7846e-06],\n",
      "        [ 2.4077e-09,  1.6524e-09,  6.6094e-10,  9.4421e-11],\n",
      "        [ 5.3314e-11,  3.6588e-11,  1.4635e-11,  2.0908e-12],\n",
      "        [ 9.2515e-07,  6.3491e-07,  2.5396e-07,  3.6280e-08],\n",
      "        [ 3.1125e-05,  2.1360e-05,  8.5440e-06,  1.2206e-06],\n",
      "        [ 2.7945e-07,  1.9178e-07,  7.6713e-08,  1.0959e-08],\n",
      "        [ 2.6185e-13,  1.7970e-13,  7.1879e-14,  1.0268e-14],\n",
      "        [ 8.0606e-06,  5.5318e-06,  2.2127e-06,  3.1610e-07],\n",
      "        [ 1.2358e-09,  8.4808e-10,  3.3923e-10,  4.8462e-11],\n",
      "        [ 8.3110e-07,  5.7036e-07,  2.2815e-07,  3.2592e-08],\n",
      "        [ 5.2954e-05,  3.6341e-05,  1.4536e-05,  2.0766e-06],\n",
      "        [ 2.9555e-08,  2.0283e-08,  8.1133e-09,  1.1590e-09],\n",
      "        [ 3.1130e+00,  2.1364e+00,  8.5455e-01,  1.2208e-01],\n",
      "        [ 2.9505e-11,  2.0249e-11,  8.0995e-12,  1.1571e-12],\n",
      "        [ 4.0457e-07,  2.7764e-07,  1.1106e-07,  1.5865e-08],\n",
      "        [ 2.9584e-08,  2.0303e-08,  8.1212e-09,  1.1602e-09],\n",
      "        [ 5.7379e-07,  3.9378e-07,  1.5751e-07,  2.2502e-08],\n",
      "        [ 2.6377e-08,  1.8102e-08,  7.2407e-09,  1.0344e-09],\n",
      "        [ 8.1644e-10,  5.6030e-10,  2.2412e-10,  3.2017e-11],\n",
      "        [ 4.7806e-06,  3.2808e-06,  1.3123e-06,  1.8748e-07],\n",
      "        [ 2.9920e-11,  2.0534e-11,  8.2135e-12,  1.1734e-12],\n",
      "        [ 1.8622e-04,  1.2780e-04,  5.1119e-05,  7.3027e-06],\n",
      "        [ 1.2893e-07,  8.8483e-08,  3.5393e-08,  5.0562e-09],\n",
      "        [ 1.8077e-09,  1.2406e-09,  4.9624e-10,  7.0891e-11],\n",
      "        [ 9.1021e-06,  6.2465e-06,  2.4986e-06,  3.5694e-07],\n",
      "        [ 2.0548e-05,  1.4102e-05,  5.6406e-06,  8.0581e-07],\n",
      "        [ 3.8205e-09,  2.6219e-09,  1.0488e-09,  1.4982e-10],\n",
      "        [ 1.2873e-06,  8.8341e-07,  3.5336e-07,  5.0480e-08],\n",
      "        [ 1.1492e-08,  7.8868e-09,  3.1547e-09,  4.5067e-10],\n",
      "        [ 1.5406e-12,  1.0573e-12,  4.2292e-13,  6.0417e-14],\n",
      "        [ 8.5432e-08,  5.8630e-08,  2.3452e-08,  3.3503e-09],\n",
      "        [ 2.2655e-06,  1.5548e-06,  6.2191e-07,  8.8844e-08],\n",
      "        [ 2.7147e-06,  1.8630e-06,  7.4521e-07,  1.0646e-07],\n",
      "        [ 1.5583e-08,  1.0694e-08,  4.2777e-09,  6.1110e-10],\n",
      "        [ 1.9841e+00,  1.3616e+00,  5.4464e-01,  7.7806e-02],\n",
      "        [ 6.5213e-06,  4.4754e-06,  1.7902e-06,  2.5574e-07],\n",
      "        [ 1.3936e-11,  9.5638e-12,  3.8255e-12,  5.4651e-13],\n",
      "        [ 2.8154e-10,  1.9321e-10,  7.7285e-11,  1.1041e-11],\n",
      "        [ 1.4100e-05,  9.6765e-06,  3.8706e-06,  5.5294e-07],\n",
      "        [ 2.2482e-06,  1.5429e-06,  6.1716e-07,  8.8166e-08],\n",
      "        [ 1.4886e-09,  1.0216e-09,  4.0862e-10,  5.8375e-11],\n",
      "        [ 3.5744e-04,  2.4530e-04,  9.8121e-05,  1.4017e-05],\n",
      "        [ 6.4942e-07,  4.4568e-07,  1.7827e-07,  2.5467e-08],\n",
      "        [ 7.7321e-11,  5.3063e-11,  2.1225e-11,  3.0322e-12],\n",
      "        [ 3.5594e-06,  2.4427e-06,  9.7708e-07,  1.3958e-07],\n",
      "        [ 5.5815e-08,  3.8305e-08,  1.5322e-08,  2.1888e-09],\n",
      "        [ 8.7416e-10,  5.9991e-10,  2.3997e-10,  3.4281e-11],\n",
      "        [ 4.8278e-07,  3.3132e-07,  1.3253e-07,  1.8932e-08],\n",
      "        [ 2.7602e-10,  1.8942e-10,  7.5769e-11,  1.0824e-11],\n",
      "        [ 5.9599e-12,  4.0901e-12,  1.6360e-12,  2.3372e-13],\n",
      "        [ 2.5605e-07,  1.7572e-07,  7.0290e-08,  1.0041e-08],\n",
      "        [ 4.3118e-08,  2.9591e-08,  1.1836e-08,  1.6909e-09],\n",
      "        [ 1.3781e-10,  9.4573e-11,  3.7829e-11,  5.4041e-12],\n",
      "        [ 5.5202e-12,  3.7884e-12,  1.5153e-12,  2.1648e-13],\n",
      "        [ 1.7463e-08,  1.1985e-08,  4.7938e-09,  6.8483e-10],\n",
      "        [ 2.4032e-10,  1.6492e-10,  6.5970e-11,  9.4243e-12],\n",
      "        [ 3.5918e-10,  2.4650e-10,  9.8598e-11,  1.4085e-11],\n",
      "        [ 3.5688e-10,  2.4492e-10,  9.7968e-11,  1.3995e-11],\n",
      "        [ 5.0185e-09,  3.4441e-09,  1.3776e-09,  1.9680e-10],\n",
      "        [ 8.8217e-05,  6.0541e-05,  2.4216e-05,  3.4595e-06],\n",
      "        [ 3.0020e-04,  2.0602e-04,  8.2409e-05,  1.1773e-05],\n",
      "        [ 6.7096e-06,  4.6047e-06,  1.8419e-06,  2.6312e-07],\n",
      "        [ 5.8221e-06,  3.9955e-06,  1.5982e-06,  2.2832e-07],\n",
      "        [ 1.4236e-08,  9.7697e-09,  3.9079e-09,  5.5827e-10],\n",
      "        [ 8.2843e-10,  5.6853e-10,  2.2741e-10,  3.2488e-11],\n",
      "        [ 1.0412e-10,  7.1455e-11,  2.8582e-11,  4.0831e-12],\n",
      "        [ 2.7227e-06,  1.8685e-06,  7.4742e-07,  1.0677e-07],\n",
      "        [ 1.9630e-09,  1.3471e-09,  5.3885e-10,  7.6979e-11],\n",
      "        [ 1.0482e-07,  7.1939e-08,  2.8775e-08,  4.1108e-09],\n",
      "        [ 1.3995e-08,  9.6042e-09,  3.8417e-09,  5.4881e-10],\n",
      "        [ 6.9476e-11,  4.7680e-11,  1.9072e-11,  2.7246e-12],\n",
      "        [ 5.0414e-06,  3.4598e-06,  1.3839e-06,  1.9770e-07],\n",
      "        [ 1.6103e-13,  1.1051e-13,  4.4205e-14,  6.3151e-15],\n",
      "        [ 9.9437e-11,  6.8241e-11,  2.7296e-11,  3.8995e-12],\n",
      "        [ 9.4070e-13,  6.4558e-13,  2.5823e-13,  3.6890e-14],\n",
      "        [ 3.9036e-07,  2.6789e-07,  1.0716e-07,  1.5308e-08],\n",
      "        [ 1.5967e-10,  1.0958e-10,  4.3831e-11,  6.2615e-12],\n",
      "        [ 1.3815e-07,  9.4811e-08,  3.7924e-08,  5.4178e-09],\n",
      "        [ 9.2785e-09,  6.3676e-09,  2.5470e-09,  3.6386e-10],\n",
      "        [ 2.6405e-04,  1.8121e-04,  7.2484e-05,  1.0355e-05],\n",
      "        [ 1.7719e-10,  1.2160e-10,  4.8642e-11,  6.9488e-12],\n",
      "        [ 7.4852e-08,  5.1369e-08,  2.0548e-08,  2.9354e-09],\n",
      "        [ 5.2084e-14,  3.5744e-14,  1.4298e-14,  2.0425e-15],\n",
      "        [ 1.2414e-08,  8.5197e-09,  3.4079e-09,  4.8684e-10],\n",
      "        [ 9.7419e-09,  6.6856e-09,  2.6742e-09,  3.8204e-10],\n",
      "        [ 1.2190e-10,  8.3660e-11,  3.3464e-11,  4.7806e-12],\n",
      "        [ 6.0222e-08,  4.1329e-08,  1.6532e-08,  2.3617e-09],\n",
      "        [ 3.0040e-12,  2.0616e-12,  8.2463e-13,  1.1780e-13],\n",
      "        [ 7.1805e-07,  4.9278e-07,  1.9711e-07,  2.8159e-08],\n",
      "        [ 1.7666e-08,  1.2124e-08,  4.8495e-09,  6.9278e-10],\n",
      "        [ 5.0711e-08,  3.4802e-08,  1.3921e-08,  1.9887e-09],\n",
      "        [ 2.2069e-08,  1.5145e-08,  6.0580e-09,  8.6543e-10],\n",
      "        [ 7.9422e-07,  5.4505e-07,  2.1802e-07,  3.1146e-08],\n",
      "        [ 6.2633e-08,  4.2983e-08,  1.7193e-08,  2.4562e-09],\n",
      "        [ 1.2760e-08,  8.7569e-09,  3.5027e-09,  5.0039e-10],\n",
      "        [ 2.4297e-09,  1.6674e-09,  6.6697e-10,  9.5282e-11],\n",
      "        [ 1.9405e-08,  1.3317e-08,  5.3270e-09,  7.6100e-10],\n",
      "        [ 2.0505e-09,  1.4072e-09,  5.6288e-10,  8.0411e-11]])\n"
     ]
    }
   ],
   "source": [
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-d7hUfs0qI_"
   },
   "source": [
    "That's pretty neat right? What's more, every time we call the .backward() method on a value we just computed, Pytorch knows that it has to calculate gradients for all tensors that played a part in that computation and these gradients are accumulated additively. All of them? I hear you ask. Well actually that would be too computationally heavy. Instead, Pytorch does this only for tensors that have been defined with the requires_grad flag set to True. So we just need to remember to set that flag for all model parameters, all tensors we would like to modify in order to minimize our loss function.\n",
    "\n",
    "All this basically means that the only thing we need to do is come up with the appropriate loss function for a dataset and problem, write down the code that computes it (known as the forward step) and Pytorch will calculate the gradients (known as the backward step). I hope you take a moment to apreciate just how wonderfully elegant the scheme is!\n",
    "\n",
    "Putting everything in a loop around the dataset we now get code that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "GNJn4Ylx1UPn",
    "outputId": "f721b1c9-0d90-4ba1-d95b-dfa1e9e452a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f768d65c110>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ls=[]\n",
    "\n",
    "W = torch.randn(C,F,requires_grad=True,dtype=torch.float32)\n",
    "alpha = 5e-4\n",
    "for e in range(1000):\n",
    "  Ltot = 0.0\n",
    "  for x,t in zip(X,T):\n",
    "    p=torch.exp(W @ x)\n",
    "    y = p/sum(p)\n",
    "    L = -torch.log(y[t])\n",
    "    L.backward() # This is where all the magic happens\n",
    "    Ltot += L.item()\n",
    "  W.data -= alpha*W.grad # gradient descent step\n",
    "  W.grad.data.zero_() # we need to remember to set the gradient to zero (pytorch)\n",
    "                      # cannot know when our loss function computation is complete\n",
    "  Ls.append(Ltot)\n",
    "plt.plot(Ls,'r-')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab2onGD_using_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
